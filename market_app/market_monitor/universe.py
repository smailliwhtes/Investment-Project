from __future__ import annotations

import csv
from pathlib import Path

import pandas as pd
import requests

from market_monitor.offline import require_online

NASDAQ_LISTED_URL = "https://www.nasdaqtrader.com/dynamic/symdir/nasdaqlisted.txt"
OTHER_LISTED_URL = "https://www.nasdaqtrader.com/dynamic/symdir/otherlisted.txt"


def _parse_pipe_table(text: str) -> pd.DataFrame:
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    header_idx = None
    for idx, line in enumerate(lines):
        if line.startswith("Symbol|") or line.startswith("ACT Symbol|"):
            header_idx = idx
            break
    if header_idx is None:
        raise ValueError("Could not find header row in universe response.")
    header = lines[header_idx].split("|")
    rows = []
    for line in lines[header_idx + 1 :]:
        if line.startswith("File Creation Time"):
            break
        parts = line.split("|")
        if len(parts) == len(header):
            rows.append(parts)
    return pd.DataFrame(rows, columns=header)


def fetch_universe() -> pd.DataFrame:
    require_online("fetch universe list")
    listed = requests.get(NASDAQ_LISTED_URL, timeout=30).text
    other = requests.get(OTHER_LISTED_URL, timeout=30).text
    df_listed = _parse_pipe_table(listed)
    df_other = _parse_pipe_table(other)
    df_listed = df_listed.rename(columns={"Symbol": "symbol", "Security Name": "name"})
    df_other = df_other.rename(columns={"ACT Symbol": "symbol", "Security Name": "name"})
    combined = pd.concat([df_listed, df_other], ignore_index=True)
    combined = combined[["symbol", "name"]].drop_duplicates()
    combined["exchange"] = None
    combined["security_type"] = "COMMON"
    combined["status"] = None
    combined["currency"] = "USD"
    return combined


def read_watchlist(path: Path) -> pd.DataFrame:
    if not path.exists():
        return pd.DataFrame(
            columns=[
                "symbol",
                "name",
                "exchange",
                "security_type",
                "status",
                "currency",
                "theme_bucket",
                "asset_type",
            ]
        )
    if path.suffix.lower() == ".csv":
        return _read_watchlist_csv(path)
    return _read_watchlist_text(path)


def _read_watchlist_text(path: Path) -> pd.DataFrame:
    symbols: list[str] = []
    with path.open("r", encoding="utf-8") as handle:
        for line in handle:
            symbol = line.strip().split("#")[0].strip()
            if symbol:
                symbols.append(symbol.upper())
    return pd.DataFrame(
        {
            "symbol": symbols,
            "name": symbols,
            "exchange": None,
            "security_type": "COMMON",
            "status": None,
            "currency": "USD",
            "theme_bucket": None,
            "asset_type": None,
        }
    )


def _has_symbol_header(path: Path) -> bool:
    """Return True if the first line of the CSV contains a 'symbol' column header."""
    with path.open("r", encoding="utf-8") as handle:
        first_line = handle.readline()
    if not first_line:
        return False
    fields = [f.strip().lower() for f in first_line.split(",")]
    return "symbol" in fields


def _read_watchlist_csv(path: Path) -> pd.DataFrame:
    # If the CSV does not have a 'symbol' header, treat it as a headerless
    # one-symbol-per-line file (e.g. generated by bootstrap without headers).
    if not _has_symbol_header(path):
        return _read_watchlist_text(path)

    required_headers = {"symbol", "theme_bucket", "asset_type"}
    rows = []
    with path.open("r", encoding="utf-8", newline="") as handle:
        reader = csv.DictReader(handle)
        if not reader.fieldnames:
            raise ValueError("Watchlist CSV is missing headers.")
        reader.fieldnames = [field.strip() for field in reader.fieldnames if field]
        headers = {field for field in reader.fieldnames if field}
        lower_headers = {field.lower() for field in headers}
        header_map = {field.lower(): field for field in reader.fieldnames if field}

        # If the CSV has a 'symbol' column but not theme_bucket/asset_type,
        # read only the symbol column and fill defaults for the rest.
        if "symbol" in lower_headers and (required_headers - lower_headers):
            for row_number, row in enumerate(reader, start=2):
                symbol = (row.get(header_map["symbol"]) or "").strip().upper()
                if not symbol:
                    continue
                theme_bucket = (row.get(header_map.get("theme_bucket", ""), "") or "").strip() if "theme_bucket" in lower_headers else ""
                asset_raw = (row.get(header_map.get("asset_type", ""), "") or "").strip() if "asset_type" in lower_headers else ""
                asset_type = asset_raw if asset_raw else "equity"
                rows.append(
                    {
                        "symbol": symbol,
                        "theme_bucket": theme_bucket,
                        "asset_type": asset_type,
                    }
                )
        else:
            # Full headered CSV with all required columns
            missing = required_headers - lower_headers
            if missing:
                missing_list = ", ".join(sorted(missing))
                raise ValueError(f"Watchlist CSV missing required headers: {missing_list}.")

            for row_number, row in enumerate(reader, start=2):
                symbol = (row.get(header_map["symbol"]) or "").strip().upper()
                if not symbol:
                    raise ValueError(f"Watchlist CSV missing symbol at row {row_number}.")
                theme_bucket = (row.get(header_map["theme_bucket"]) or "").strip()
                asset_raw = (row.get(header_map["asset_type"]) or "").strip()
                asset_type = _normalize_asset_type(asset_raw, row_number)
                rows.append(
                    {
                        "symbol": symbol,
                        "theme_bucket": theme_bucket,
                        "asset_type": asset_type,
                    }
                )

    if not rows:
        return pd.DataFrame(
            columns=[
                "symbol",
                "name",
                "exchange",
                "security_type",
                "status",
                "currency",
                "theme_bucket",
                "asset_type",
            ]
        )

    df = pd.DataFrame(rows)
    security_type = df["asset_type"].map(
        lambda asset: "ETF" if asset in {"ETF", "ETN"} else "COMMON"
    )
    df = df.assign(
        name=df["symbol"],
        exchange=None,
        security_type=security_type,
        status=None,
        currency="USD",
    )
    return df[
        [
            "symbol",
            "name",
            "exchange",
            "security_type",
            "status",
            "currency",
            "theme_bucket",
            "asset_type",
        ]
    ]


def _normalize_asset_type(value: str, row_number: int) -> str:
    if not value:
        raise ValueError(f"Watchlist CSV missing asset_type at row {row_number}.")
    normalized = value.strip().lower()
    allowed = {
        "etf": "ETF",
        "equity": "equity",
        "trust": "trust",
        "etn": "ETN",
    }
    if normalized not in allowed:
        allowed_list = ", ".join(sorted(allowed.values()))
        raise ValueError(
            f"Invalid asset_type '{value}' at row {row_number}. Allowed values: {allowed_list}."
        )
    return allowed[normalized]


def filter_universe(
    df: pd.DataFrame,
    allowed_types: list[str],
    allowed_currencies: list[str],
    include_etfs: bool,
) -> pd.DataFrame:
    filtered = df.copy()
    if allowed_types:
        filtered = filtered[filtered["security_type"].isin(allowed_types)]
    if allowed_currencies:
        filtered = filtered[filtered["currency"].isin(allowed_currencies)]
    if not include_etfs:
        filtered = filtered[filtered["security_type"] != "ETF"]
    return filtered.reset_index(drop=True)


def write_universe_csv(df: pd.DataFrame, path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(path, index=False)
